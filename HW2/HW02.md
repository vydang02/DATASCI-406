HW02
================
Vy Dang, ivydang
2025-02-18

## Question 1 (4 points)

The [Monty Hall
Problem](https://en.wikipedia.org/wiki/Monty_Hall_problem) is a classic
statistical pu \> Suppose you’re on a game show, and you’re given the
choice of three doors: Behind one door is a car;zzle. Borrowing from a
famous article on the subject: behind the others, goats. You pick a
door, say No. 1, and the host, who knows what’s behind the doors, opens
another door, say No. 3, which has a goat. He then says to you, “Do you
want to pick door No. 2?” Is it to your advantage to switch your choice?

Along with this basic framework, assume:

- The host must always open a door that was not selected by the
  contestant.
- The host must always open a door to reveal a goat and never the car.
- The host must always offer the chance to switch between the door
  chosen originally and the closed door remaining.

## Question 1.a (1 point)

Write up the Monty Hall problem in terms of random variables and verify
the choice of switching has a 2/3 probability of winning the car.

> Frame the problem in terms of conditional probability: A: Event the
> car is behind your initially chosen door B: Event the car is behind
> one of other doors D_i: Event that the car is behind door i, i in
> {1,2,3} H: Event that host opens a door that has a goat behind

> We have: P(D1) = P(D2) = P(D3) = 1/3

> After host opens door revealing a goat, we have following
> probabilities: Case 1: Contestant swicthes their choice: - If the car
> behind door 1: P(D1 \| Goat behind door 3) = P(D1) = 1/3 - If the car
> behind door 2 or door 3: + P(D2 \| Goat behind door 3) = \[P(Goat
> behind door 3 \| D2) \* P(D2)\] / P(Goat behind door 3)

> Since host only shows the goat when D2 is true, we have: - P(Goat
> behind door 3 \| D2) = 1 - P(Goat behind door 3 \| D3) = 0

> Case 2: Contestant doesn’t switch their choice: - If car behind
> initially chosen door: P = 1/3 - If car behind one of other doors: P =
> 2/3 (as host actions are irrelevant to inital probabilities)

> =\> P(win by switching) = P(car behind one of other doors) = 1 - P(car
> behind initially chosen door) = 1-1/3 = 2/3 (because if car behind
> initially chosen door, switching leads to losing)

## Question 1.b (1 point)

Here are 10,000 replications of selecting a random door to put the car
behind.

``` r
set.seed(123)
cars <- sample(1:3, 10000, replace = TRUE)
initial_choices <- sample(1:3, 10000, replace = TRUE)
host_reveals <- numeric(10000)
for (i in 1:10000){
  possible_doors <- setdiff(1:3, c(initial_choices[i], cars[i]))
  host_reveals[i] <- sample(possible_doors,1)
}
remaining_doors <- 6 - initial_choices - host_reveals
win_by_switching <- remaining_doors == cars
```

For each choice of door, run the Monty Hall problem 10,000 times and
calculate the proportion of times the contestant wins the car when
switching doors.

Hints:

- when calculating the final probability of winning the car, you can use
  the `mean` function to calculate the proportion of times the
  contestant wins the car when switching doors. For example, here’s the
  simulated probablility of randomly selecting the car on the initial
  guess:

``` r
mean(sample(1:3, 10000, replace = TRUE) == cars)
```

    ## [1] 0.3307

``` r
mean(win_by_switching)
```

    ## [1] 0.4137

- the `if_else` function can be useful to select between values based on
  a condition. Here’s an example:

``` r
x <- 1:10
# for the values 5 to 10, double those values, otherwise return the original value
if_else(x < 5, x, 2*x)
```

    ##  [1]  1  2  3  4 10 12 14 16 18 20

## Question 1.c (2 point)

Consider a variant of the Monty Hall problem where if the user picks the
car in the first round, with probability $p$, the host will immediately
reveal that the car is behind the door the user selected and with
probability $1-p$ will open another door with a goat.

Update your simulation to calculate the probability of winning the car
when switching doors in this variant of the Monty Hall problem with
$p = 0.25$.

``` r
set.seed(123)
p <- 0.25
n_simulations <- 10000
cars <- sample(1:3, n_simulations, replace = TRUE)
car_initial_choice <- initial_choices == cars
reveal_car <- sample(c(TRUE,FALSE), n_simulations, replace = TRUE, prob = c(p,1-p))
host_reveals <- numeric(n_simulations)
for (i in 1:n_simulations) {
  if (car_initial_choice[i] & reveal_car[i]) {
    host_reveals[i] <- NA
  } else {
    possible_doors <- setdiff(1:3, c(initial_choices[i], cars[i]))
    host_reveals[i] <- sample(possible_doors, 1)
  }
}

remaining_doors <- ifelse(is.na(host_reveals), initial_choices, 6 - initial_choices - host_reveals)
switch_wins <- remaining_doors == cars
switch_wins[is.na(host_reveals)] <- FALSE
mean(switch_wins)
```

    ## [1] 0.4143

Here is how you can sample two options with specific probability:

``` r
sample(c(TRUE, FALSE), 10, replace = TRUE, prob = c(0.25, 0.75))
```

    ##  [1] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE

## Question 2 (5 points)

Let $U_1$ and $U_1$ be two independent random variables that are
uniformly distributed on the interval $[0, 1]$. Let $X = U_1 + U_2$.

## Question 2.a (2 points)

Find the CDF of $X$.

> Joint PDF of U1 and U2:

fU1,U1(u1,u2) = $$
 f_{U_1, U_2}(u_1, u_2) = 1 \quad \text{for } 0 \le u_1 \le 1 \text{ and } 0 \le u_2 \le 1 
$$

For $$0 \le x \le 1:  f_X(x) = \int_0^x 1 , du_1 = x $$ with u1 ranges
from 0 to x, and u2= x-u1 ranges from 0 to 1

For $$( 1 < x \le 2 ): [ f_X(x) = \int_{x-1}^1 1 , du_1 = 2 - x ]$$ with
u1 ranges from $$x-1$$ to 1 and u2 = $$x-u1$$ ranges from 0 and 1

So PDF of X is
$$f_X(x) = \begin{cases} x & \text{if } 0 \le x \le 1, \ 2 - x & \text{if } 1 < x \le 2, \ 0 & \mathrm{otherwise} \end{cases} $$

For $$ x < 0 :  F_X(x) = 0 $$ For
$$ 0 \le x \le 1 :  F_X(x) = \int_0^x t , dt = \left[ \frac{t^2}{2} \right]_0^x = \frac{x^2}{2} $$
For
$$ 1 < x \le 2 :  F_X(x) = \int_0^1 t , dt + \int_1^x (2 - t) , dt = \left[ \frac{t^2}{2} \right]_0^1 + \left[ 2t - \frac{t^2}{2} \right]_1^x = \frac{1}{2} + \left( 2x - \frac{x^2}{2} \right) - \left( 2 - \frac{1}{2} \right) = \frac{1}{2} + \left( 2x - \frac{x^2}{2} \right) - \frac{3}{2} = 2x - \frac{x^2}{2} - 1 $$

So the CDF of X is:
$$ F_X(x) = \begin{cases} 0 & \text{if } x < 0, \ \frac{x^2}{2} & \text{if } 0 \le x \le 1, \ 1 - \frac{(2-x)^2}{2} & \text{if } 1 < x \le 2, \ 1 & \text{if } x > 2 \end{cases} $$
\## Question 2.b (1 point)

Using the `runif` function, generate 10,000 draws of $U_1$. Then
generate 10,000 draws of $U_2$. Calculate the values of $X$ and $Y$ for
each pair of draws. Plot results using `plot(ecdf(RESULT))`, where
RESULT is the 10,000 values of $X$ or $Y$.

``` r
n_draws <- 10000
set.seed(123)
U1 <- runif(n_draws, min = 0, max = 1)
U2 <- runif(n_draws, min = 0, max = 1)
X <- U1 + U2
plot(ecdf(X))
```

![](HW02_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

## Question 2.c (1 point)

Take the derivative of the CDF from 2.a to get the PDF of $X$.

For $$ x < 0 :  f_X(x) = \frac{d}{dx}0 = 0 $$ For
$$ 0 \le x \le 1 :  f_X(x) = \frac{d}{dx} \left[ \frac{x^2}{2} \right] = x $$
For
$$ 1 < x \le 2 : f_X(x) = \frac{d}{dx} \left[ 1 - \frac{(2-x)^2}{2} \right] = \frac{d}{dx} \left[ 1 - \left(\frac{4 - 4x + x^2}{2} \right) \right] = \frac{d}{dx} \left[ 1 - 2 + 2x - \frac{x^2}{2} \right] = \frac{d}{dx} \left[ -1 + 2x - \frac{x^2}{2} \right] = 2 - x $$
For $$ x > 2 :  f_X(x) = \frac{d}{dx}1 = 0 $$ So PDF of $X$ is
$$ f_X(x) = \begin{cases} 0 & \text{if } x < 0, \ x & \text{if } 0 \le x \le 1, \ 2 - x & \text{if } 1 < x \le 2, \ 0 & \text{if } x > 2 \end{cases} $$

## Question 2.d (1 point)

Find the mean and variance of $X$ analytically. Verify your results by
comparing it to the mean and variance you would calculate from the
10,000 draws of $X$.

Mean of $U1$ and $U2$:
$$ \mathbb{E}[U_1] = \mathbb{E}[U_2] = \frac{0+1}{2} = \frac{1}{2} $$
Mean of $X$: By linearity of expectation:
$$\mathbb{E}[X] = \mathbb{E}[U_1 + U_2] = \mathbb{E}[U_1] + \mathbb{E}[U_2] = \frac{1}{2} + \frac{1}{2} = 1$$
Variance of $U1$ and $U2$:
$$\mathrm{Var}(U_1) = \mathrm{Var}(U_2) = \frac{(1-0)^2}{12} = \frac{1}{12}$$
Variance of $X$: Since $U1$ and $U2$ are independent,
$$\mathrm{Var}(X) = \mathrm{Var}(U_1 + U_2) = \mathrm{Var}(U_1) + \mathrm{Var}(U_2) = \frac{1}{12} + \frac{1}{12} = \frac{1}{6}$$

``` r
n_draws <- 10000
set.seed(123)
U1 <- runif(n_draws, min = 0, max = 1)
U2 <- runif(n_draws, min = 0, max = 1)
X <- U1 + U2
mean_X <- mean(X)
mean_X
```

    ## [1] 0.9938187

``` r
var_X <- var(X)
var_X
```

    ## [1] 0.1623921

## Question 3 (6 points)

Consider a variation on the uniform distribution. Let $X$ be a random
variable with the following PDF:

$$f(x) = \frac{1}{\theta} \text{ for } 0 \leq x \leq \theta$$ Suppose
that $X_1, X_2, \ldots, X_n$ are independent and identically distributed
random variables with this PDF.

While the output is suppressed, the next chunk defines a vector `x` that
contains a sample of 1000 random variables from this distribution.

![](HW02_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

### Question 3.a (2 point)

Find the **methods of moments** estimator of $\theta$ using the sample
$X_1, X_2, \ldots, X_n$.

We start by computing the first moment of $X$:

$$E(X) = \int_0^\theta \frac{x}{\theta} \, dx = \left. \frac{x^2}{2\theta} \right|_{x = 0}^\theta = \frac{\theta}{2}$$

Then solve $E(X) = \theta/2$ for $\theta$, to get $\theta = 2 E(X)$.
Substituting $\bar X$ for $E(X)$, $\hat \theta = 2 \bar X$.

Population mean (first moment):
$$E(X) = \int_0^\theta x f(x) , dx = \int_0^\theta x \left(\frac{1}{\theta}\right) dx = \frac{1}{\theta} \int_0^\theta x , dx = \frac{1}{\theta} \cdot \left. \frac{x^2}{2} \right|_0^\theta = \frac{1}{\theta} \cdot \frac{\theta^2}{2} = \frac{\theta}{2}$$
We have sample mean equals to population mean:
$$\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i$$ So
$$ \bar{X} = \frac{\theta}{2}$$. Solve for $\theta$:
$$\theta = 2 \bar{X}$$ =\> $$\hat{\theta} = 2 \bar{X}$$ The method of
moments estimator for $\theta$ using the sample
$$X_1, X_2, \ldots, X_n$$ is $$\hat{\theta} = 2 \bar{X}$$. \### Question
3.b (2 point)

For a given $\theta$ value, implement the **likelihood function**:

``` r
x <- c(1.12697961321101, 5.45682713272981, 4.77657136018388, 3.29002340068109, 
0.123430412728339, 3.67342025274411, 3.09185637696646, 2.74907605885528, 
3.16958140116185, 0.384938379516825, 2.36123094917275, 4.07640412915498, 
1.78495561610907, 1.25764413527213, 5.60947886947542, 0.0126053700223565, 
4.41430675797164, 4.84060383844189)
likelihood <- function(theta) {
  log_likelihood <- ifelse(
    all(x >= 0 & x <= theta),
    -length(x) * log(theta),
    -Inf
  )
  return(exp(log_likelihood)) 
}
```

Plot the likelihood for $\theta \in [0, 20]$:

``` r
## the "x" below is actually our theta parameter
## the curve function is a special function that needs an expression involving "x"
## The "Vectorize" function allows us to take many theta values simultaneously
curve(Vectorize(likelihood)(x), from = 0, to  = 20, ylab = "Likelihood", xlab = expression(theta))
```

![](HW02_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

What do you notice? Where is the highest point of the plot?

The highest point of the plot is roughly at $\theta$ equals 5.6 or 6.
Here, the plot peaks at the maximum observed value in the sample (here I
initialized a small subset of x instead of using the whole vector).

### Part (c) (2 pt)

Find the **maximum likelihood estimator** for $\theta$ as a function of
the data. Compute the MLE estimate from `x` given in at the start of the
problem. Compare it to your graph. Do the two solutions agree?

The likelihood function for a uniform distribution
$$X \sim U(0, \theta):  L(\theta) = \prod_{i=1}^n f(x_i; \theta)  f(x; \theta) = \frac{1}{\theta} \text{ for } 0 \leq x \leq \theta$$
We have the likelihood function as:
$$ L(\theta) = \left( \frac{1}{\theta} \right)^n \qquad \text{ if } 0 \leq x_i \leq \theta \text{ for all } i$$
=\> $$ L(\theta) = 0 \text{ if any } x_i > \theta$$ So the MLE for
$\theta$ is $$ \hat{\theta}_{MLE} = \max(X_1, X_2, \ldots, X_n)$$

``` r
theta_MLE <- max(x)
theta_MLE
```

    ## [1] 5.609479

The two solutions agree with MLE of $\theta$ is approximately 5.6.

## Question 4 (5 points)

Consider the Poisson distribution with probability mass function:

$$p(x) = \frac{e^{-\lambda} \lambda^x}{x!}$$ for $\lambda > 0$.

The Poisson distribution has four related functions in R which we can
use:

- `dpois`: the probability mass function
- `ppois`: the cumulative distribution function
- `qpois`: the quantile function (inverse cdf)
- `rpois`: generate random deviates (random numbers that follow this
  distribution)

### Question 4.a (1 point)

``` r
library(dplyr)
library(ggplot2)
```

``` r
lambda <- 3
x <- data.frame(x = 0:10)
```

Using the mutate function, add 2 new columns to the data frame `x`:

- `d`: The PMF evaluated at each `x` value.
- `p`: The CDF evaluated at each `x` value.

``` r
x <- x |> mutate(
  d = dpois(x, lambda),
  p = ppois(x, lambda)
)
x
```

    ##     x            d          p
    ## 1   0 0.0497870684 0.04978707
    ## 2   1 0.1493612051 0.19914827
    ## 3   2 0.2240418077 0.42319008
    ## 4   3 0.2240418077 0.64723189
    ## 5   4 0.1680313557 0.81526324
    ## 6   5 0.1008188134 0.91608206
    ## 7   6 0.0504094067 0.96649146
    ## 8   7 0.0216040315 0.98809550
    ## 9   8 0.0081015118 0.99619701
    ## 10  9 0.0027005039 0.99889751
    ## 11 10 0.0008101512 0.99970766

Graph the PMF and CDF for the Poisson distribution with $\lambda = 3$
using the data frame `x`. The `geom_col` will be helpful for the PMF.
The `geom_step` function is useful for the CDF.

``` r
#ggplot(x, aes(x = x, y = d)) + geom_col()
ggplot(x, aes(x = x, y = p)) + geom_step()
```

![](HW02_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->

### Question 4.b (2 points)

Suppose that we have observed $N$ events in a fixed interval of time.
The time between events is distributed as an exponential distribution
with rate parameter $\lambda$:

$$f(x) = \lambda e^{-\lambda x}$$

for $x > 0$ and as before $\lambda > 0$. Notice that $N$ is a random
variable, in particular a Poisson distributed random variable.

Suppose that we want to test a hypothesis about $\lambda$. It would be
natural to use the total number of observed events as a test statistic.

If our null hypothesis is that $\lambda = 3$ and our alternative is that
$\lambda = 4$, graph both the null and alternative distributions of the
test statistic. Looking at the plot, where would be a good place to
locate an interval of the form $[a, \infty)$ or \$\[0, a\], such that
this interval would have good power?

``` r
lamda_null <- 3
lamda_alt <- 4
x_values <- 0:15
pmf_null <- dpois(x_values, lamda_null)
pmf_alt <- dpois(x_values, lamda_alt)
data <- data.frame(
  x = rep(x_values, 2),
  pmf = c(pmf_null, pmf_alt),
  hypothesis = rep(c("Null (λ=3)", "Alternative (λ=4)"), each = length(x_values))
)
```

``` r
ggplot(data, aes(x=x, y=pmf, fill = hypothesis))
```

![](HW02_files/figure-gfm/unnamed-chunk-17-1.png)<!-- -->

### Question 4.c (1 point)

Using `qpois` find the interval $[a, \infty)$ or $[0, a]$ that would
have a significance level of 0.05 with the most power for the null
hypothesis $\lambda = 3$ against the alternative that $\lambda = 4$.

### Question 4.c (1 point)

``` r
# the next line guarantees that the random numbers we generate will always be the same.
set.seed(406406406)
y <- rpois(1, lambda = 3)
```

Suppose we observe that $Y = 2$. Using the interval you found in the
previous part, test the null hypothesis $\lambda = 3$ against the
alternative that $\lambda = 4$. What is your conclusion?
